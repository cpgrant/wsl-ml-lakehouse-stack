# Project: wsl-ml-stack
# Usage: put these files in a new folder (e.g., wsl-ml-stack/) and run `make up`

# ────────────────────────────────────────────────────────────────────────────────
# docker-compose.yml
# ────────────────────────────────────────────────────────────────────────────────
version: "3.9"

x-common-env: &common_env
  TZ: ${TZ}
  PYTHONDONTWRITEBYTECODE: "1"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  PIP_NO_CACHE_DIR: "1"

services:
  # -------------------- MinIO (S3-compatible object store) --------------------
  minio:
    image: minio/minio:RELEASE.2025-01-20T00-00-00Z
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    volumes:
      - minio_data:/data
    networks: [data]

  # Optional helper to pre-create buckets
  minio-mc:
    image: minio/mc:RELEASE.2025-01-20T00-00-00Z
    container_name: minio-mc
    depends_on: [minio]
    entrypoint: ["/bin/sh","-c"]
    command: >-
      "mc alias set local http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&
       mc mb -p local/${MINIO_BUCKET} || true &&
       tail -f /dev/null"
    networks: [data]

  # ------------------------------ Kafka (KRaft) -------------------------------
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,PLAINTEXT_HOST://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_NUM_PARTITIONS=1
    ports:
      - "29092:29092"  # Host access to broker
    volumes:
      - kafka_data:/bitnami/kafka
    networks: [data]

  # ------------------------------ Spark (3.5.x) -------------------------------
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_JARS_PACKAGES=io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - AWS_REGION=${AWS_REGION}
      - AWS_S3_ENDPOINT=http://minio:9000
      - AWS_S3_ALLOW_UNSAFE_RENAME=true
      - PYSPARK_PYTHON=python3
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - workspace:/workspace
    networks: [data]

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on: [spark-master]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_JARS_PACKAGES=io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - AWS_S3_ENDPOINT=http://minio:9000
      - AWS_REGION=${AWS_REGION}
      - PYSPARK_PYTHON=python3
    ports:
      - "8081:8081"
    volumes:
      - workspace:/workspace
    networks: [data]

  # ------------------------------- Ray (2.x) ----------------------------------
  ray-head:
    image: rayproject/ray:2.34.0-py311
    container_name: ray-head
    command: ["ray","start","--head","--dashboard-host=0.0.0.0","--num-cpus","4"]
    environment:
      <<: *common_env
    ports:
      - "8265:8265"   # Ray dashboard
      - "10001:10001" # Ray client server
    volumes:
      - workspace:/workspace
    networks: [data]

  # ------------------------------- Airflow 2 ----------------------------------
  airflow-db:
    image: postgres:16
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_db:/var/lib/postgresql/data
    networks: [data]

  airflow-redis:
    image: redis:7
    container_name: airflow-redis
    networks: [data]

  airflow:
    image: apache/airflow:2.9.3-python3.11
    container_name: airflow
    depends_on: [airflow-db, airflow-redis]
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
      AIRFLOW__WEBSERVER__RBAC: "true"
      _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-apache-spark apache-airflow-providers-apache-kafka apache-airflow-providers-amazon==8.12.0 delta-spark==3.2.0"
      AIRFLOW_UID: ${AIRFLOW_UID}
      <<: *common_env
    user: "${AIRFLOW_UID}:0"
    command: ["bash","-c","airflow db upgrade && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && airflow webserver & airflow scheduler"]
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - workspace:/workspace
    ports:
      - "8085:8080"
    networks: [data]

  # -------------------------------- dbt Core ----------------------------------
  dbt:
    image: ghcr.io/dbt-labs/dbt-core:1.8.7
    container_name: dbt
    working_dir: /workspace/dbt
    entrypoint: ["tail","-f","/dev/null"]
    volumes:
      - workspace:/workspace
    networks: [data]

  # ------------------------------- Terraform ----------------------------------
  terraform:
    image: hashicorp/terraform:1.9
    container_name: terraform
    working_dir: /workspace/infra
    entrypoint: ["tail","-f","/dev/null"]
    volumes:
      - workspace:/workspace
    networks: [data]

  # --------------------------------- Beam -------------------------------------
  beam:
    image: apache/beam_python3.11_sdk:2.57.0
    container_name: beam
    working_dir: /workspace/beam
    entrypoint: ["tail","-f","/dev/null"]
    volumes:
      - workspace:/workspace
    networks: [data]

  # ------------------------------- PyDev (Jupyter) -----------------------------
  pydev:
    image: jupyter/pyspark-notebook:spark-3.5.1
    container_name: pydev
    environment:
      JUPYTER_TOKEN: ${JUPYTER_TOKEN}
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_REGION: ${AWS_REGION}
      AWS_S3_ENDPOINT: http://minio:9000
      PYSPARK_PYTHON: python3
      SPARK_OPTS: "--packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4"
    ports:
      - "8888:8888"
    command: start-notebook.sh --NotebookApp.token=${JUPYTER_TOKEN} --NotebookApp.password="" --NotebookApp.allow_origin='*'
    volumes:
      - workspace:/home/jovyan/work
    networks: [data]

networks:
  data:
    driver: bridge

volumes:
  workspace:
  minio_data:
  airflow_db:
  kafka_data:

# ────────────────────────────────────────────────────────────────────────────────
# .env  (copy this to .env in the same folder)
# ────────────────────────────────────────────────────────────────────────────────
# Timezone
TZ=Europe/Copenhagen

# MinIO
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123
MINIO_BUCKET=data
AWS_REGION=eu-north-1

# Airflow requires a Linux UID; on WSL, 50000 is usually fine
AIRFLOW_UID=50000

# Jupyter token (set your own!)
JUPYTER_TOKEN=letmein

# ────────────────────────────────────────────────────────────────────────────────
# Makefile
# ────────────────────────────────────────────────────────────────────────────────
SHELL := /bin/bash

up:
	docker compose up -d --build

ps:
	docker compose ps

logs:
	docker compose logs -f --tail=200

stop:
	docker compose stop

down:
	docker compose down -v

restart: down up

# Create a Kafka topic named 'events'
kafka-topic:
	docker compose exec -T kafka kafka-topics.sh --create --topic events --bootstrap-server kafka:9092 --if-not-exists

# Quick Spark shell with Delta & S3 (MinIO)
spark-shell:
	docker compose exec -it spark-master spark-shell \
	  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
	  --conf spark.hadoop.fs.s3a.endpoint=$$AWS_S3_ENDPOINT \
	  --conf spark.hadoop.fs.s3a.path.style.access=true

# Open Ray dashboard
ray-open:
	@echo "Ray Dashboard: http://localhost:8265"

# Airflow quick links
airflow-open:
	@echo "Airflow Web:   http://localhost:8085  (admin/admin)"

# Jupyter quick link
jupyter-open:
	@echo "Jupyter:       http://localhost:8888  (token in .env: $$JUPYTER_TOKEN)"

# ────────────────────────────────────────────────────────────────────────────────
# airflow/dags/example_kafka_to_delta.py  (minimal demo DAG)
# ────────────────────────────────────────────────────────────────────────────────
# Place this file at airflow/dags/example_kafka_to_delta.py
from datetime import datetime
from airflow import DAG
from airflow.providers.apache.kafka.sensors.kafka import KafkaSensor
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

default_args = {"owner": "airflow"}

dag = DAG(
    dag_id="example_kafka_to_delta",
    default_args=default_args,
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False,
)

wait_for_kafka = KafkaSensor(
    task_id="wait_for_kafka",
    topics=["events"],
    kafka_config={"bootstrap.servers": "kafka:9092"},
    dag=dag,
)

spark_job = SparkSubmitOperator(
    task_id="spark_stream_to_delta",
    application="/workspace/jobs/stream_to_delta.py",
    conn_id="spark_default",
    packages="io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4",
    conf={
        "spark.hadoop.fs.s3a.endpoint": "http://minio:9000",
        "spark.hadoop.fs.s3a.path.style.access": "true",
        "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension",
        "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog",
    },
    dag=dag,
)

wait_for_kafka >> spark_job

# ────────────────────────────────────────────────────────────────────────────────
# /workspace/jobs/stream_to_delta.py  (sample Spark Structured Streaming job)
# ────────────────────────────────────────────────────────────────────────────────
# Put this under the shared 'workspace' volume: jobs/stream_to_delta.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StructField, StringType

spark = (
    SparkSession.builder.appName("KafkaToDelta")
    .config("spark.sql.extensions","io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog","org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()
)

schema = StructType([StructField("message", StringType(), True)])

df = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers","kafka:9092")
    .option("subscribe","events")
    .option("startingOffsets","earliest")
    .load()
)

json_df = df.selectExpr("CAST(value AS STRING) as raw").select(from_json(col("raw"), schema).alias("js")).select("js.*")

# Write to Delta on MinIO S3 bucket
query = (
    json_df.writeStream
    .format("delta")
    .option("checkpointLocation","s3a://data/checkpoints/kafka_to_delta/")
    .option("path","s3a://data/delta/events/")
    .outputMode("append")
    .start()
)

query.awaitTermination()

# ────────────────────────────────────────────────────────────────────────────────
# /workspace/notebooks/quickstart.ipynb (create your own Jupyter notebook)
# ────────────────────────────────────────────────────────────────────────────────
# Example PySpark snippet you can paste into a Jupyter cell (pydev container):
#
# from pyspark.sql import SparkSession
# spark = (SparkSession.builder
#          .appName("DeltaQuickstart")
#          .config("spark.sql.extensions","io.delta.sql.DeltaSparkSessionExtension")
#          .config("spark.sql.catalog.spark_catalog","org.apache.spark.sql.delta.catalog.DeltaCatalog")
#          .config("spark.hadoop.fs.s3a.endpoint","http://minio:9000")
#          .config("spark.hadoop.fs.s3a.path.style.access","true")
#          .getOrCreate())
# spark.range(0,10).write.format("delta").mode("overwrite").save("s3a://data/delta/demo")
# spark.read.format("delta").load("s3a://data/delta/demo").show()
#
# # Parquet example:
# spark.range(0,10).write.mode("overwrite").parquet("s3a://data/parquet/demo")
# spark.read.parquet("s3a://data/parquet/demo").show()

# ────────────────────────────────────────────────────────────────────────────────
# Quickstart (WSL + Docker Desktop)
# ────────────────────────────────────────────────────────────────────────────────
# 1) Ensure Docker Desktop is running and WSL2 integration is enabled for Ubuntu.
# 2) Save the files above in a new folder: wsl-ml-stack/ (create airflow/dags/ dir).
# 3) Run: `make up`  → this launches everything.
# 4) Create a Kafka topic: `make kafka-topic`.
# 5) Open UIs:
#    - MinIO Console: http://localhost:9001  (login: from .env)
#    - Jupyter (PySpark): http://localhost:8888  (token in .env)
#    - Spark Master UI:   http://localhost:8080
#    - Airflow Web:       http://localhost:8085  (admin/admin)
#    - Ray Dashboard:     http://localhost:8265
# 6) Produce a Kafka message (from host):
#    docker compose exec -T kafka kafka-console-producer.sh --bootstrap-server kafka:9092 --topic events <<< '{"message":"hello"}'
# 7) Trigger the Airflow DAG `example_kafka_to_delta` (unpause & play) to stream from Kafka to Delta on MinIO.
# 8) Explore Delta/Parquet data from Jupyter or Spark shell (see Makefile targets).

# Notes
# - dbt: mount your dbt project into /workspace/dbt and run commands: 
#     docker compose exec -it dbt bash -lc "dbt --version && dbt debug"
#   (install appropriate adapters, e.g., dbt-postgres, dbt-snowflake, etc.)
# - Terraform: place IaC in /workspace/infra, then: 
#     docker compose exec -it terraform bash -lc "terraform init && terraform plan"
# - Beam: put Python pipelines in /workspace/beam and run: 
#     docker compose exec -it beam bash -lc "python your_pipeline.py --runner=DirectRunner"
# - Ray: inside pydev or ray-head, `python -c 'import ray; ray.init("ray://ray-head:10001"); print(ray.cluster_resources())'`
